{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN647 Week 3 Workshop\n",
    "## Preprocessing: Stemming and Python Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: \n",
    "Update Task 3 of last week's workshop to print the\n",
    "terms of the document and their frequency in ascending\n",
    "order. Note that dictionaries cannot be sorted, but you can\n",
    "get a representation of a dictionary that is sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used from Task 3 of the previous week\n",
    "def parse_doc(input_path, stop_words):\n",
    "    my_file = open(input_path, 'r')\n",
    "\n",
    "    stopwords_file = open(stop_words, 'r')\n",
    "    stop_words_list = stopwords_file.readlines()\n",
    "    stopwords_file.close()\n",
    "\n",
    "    stop_words_list = stop_words_list[0].split(',')\n",
    "\n",
    "    word_count = 0\n",
    "    docid = ''\n",
    "    curr_doc = {}\n",
    "\n",
    "    start_end = False\n",
    "    parsed_text = []\n",
    "    \n",
    "\n",
    "    file_ = my_file.readlines()\n",
    "\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith('<text>'):\n",
    "            start_end = True\n",
    "        if line.startswith('<newsitem '):\n",
    "            for part in line.split():\n",
    "                if part.startswith('itemid='):\n",
    "                    docid = part.split('=')[1].split('/')[0]\n",
    "                    docid = docid.replace('\"', '')\n",
    "        elif line.startswith('<p>'):\n",
    "            line = line.replace('<p>', '').replace('</p>', '')\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "            line = line.replace('quot', '')\n",
    "        elif line.startswith('</text>'):\n",
    "            start_end = False\n",
    "        if start_end:\n",
    "            parsed_text.append(line)\n",
    "\n",
    "    split_text = []\n",
    "\n",
    "    for line in parsed_text:\n",
    "        for word in line.split(): \n",
    "            word_count += 1\n",
    "\n",
    "            if word.lower() not in stop_words_list and not word.isdigit():\n",
    "                word = word.lower()\n",
    "                split_text.append(word)\n",
    "\n",
    "    split_text.remove('<text>')\n",
    "    for word in split_text:\n",
    "        if word not in curr_doc:\n",
    "            curr_doc[word] = 1\n",
    "        else:\n",
    "            curr_doc[word] += 1\n",
    "    \n",
    "    my_file.close()\n",
    "    return (word_count, docid, curr_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addition': 1, 'aires': 1, 'amid': 1, 'argentinas': 2, 'argentine': 1, 'awaiting': 1, 'axel': 1, 'bank': 1, 'between': 1, 'bocon': 1, 'bonds': 1, 'bounce': 2, 'buenos': 1, 'bugge': 1, 'change': 1, 'congress': 1, 'deficit': 1, 'delegation': 1, 'dollardenominated': 1, 'due': 2, 'during': 1, 'early': 1, 'economic': 1, 'economy': 1, 'events': 1, 'expect': 1, 'expected': 2, 'fernandez': 1, 'fiscal': 1, 'foreign': 1, 'frb': 1, 'friday': 1, 'fund': 1, 'general': 1, 'governments': 1, 'higher': 1, 'including': 1, 'international': 1, 'large': 1, 'low': 1, 'market': 1, 'marketmoving': 1, 'measures': 1, 'meeting': 1, 'minister': 1, 'monetary': 1, 'much': 1, 'new': 1, 'news': 1, 'newsroom': 1, 'now': 1, 'october': 1, 'opening': 1, 'passage': 1, 'percent': 1, 'pointing': 1, 'previsional': 1, 'prices': 1, 'roque': 1, 'rose': 2, 'session': 1, 'slight': 1, 'slightly': 1, 'small': 1, 'technical': 2, 'through': 1, 'trader': 2, 'traders': 1, 'uncertainty': 1, 'until': 1, 'volume': 1, 'waiting': 1, 'wednesday': 1}\n"
     ]
    }
   ],
   "source": [
    "doc_terms = parse_doc('wk3_workshop_data/6146.xml', 'wk3_workshop_data/common-english-words.txt')[2]\n",
    "\n",
    "{k : v for k, v in sorted(doc_terms.items(), key=lambda item: item[1])}\n",
    "\n",
    "sorted_list_ascending = {k: v for k, v in sorted(doc_terms.items(), reverse = False)}\n",
    "print(sorted_list_ascending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: \n",
    "\n",
    "Stemming refers to a crude heuristic process that removes the ends of words in the hope of finding the stemmed common base form correctly most of the time. This process often includes the removal of derivational affixes. Lovins (1968) defines a stemming algorithm as \"a procedure to reduce all words with the same stem to a common form, usually by stripping each word of its derivational and inflectional suffixes (and sometimes prefixes)\". A popular stemming algorithm is the Porter2 (Snowball) algorithm. You can read the details of Porter2 algorithm in the following link: http://snowball.tartarus.org/algorithms/english stemmer.html \n",
    "\n",
    "For Python, please go to https://pypi.python.org/pypi/stemming/1.0 to download Python implementations of porter2 stemming algorithms, follow the instruction to import and use stemmer in your Python code (or see the Blackboard). \n",
    "\n",
    "Use porter2 stemming algorithm to update your last week function parse_doc(input, stops) to make sure all terms (words) are stemmed. Then display documentâ€™s stems and their frequencies in ascending order as you did in Task 1. You can compare the outcomes of Task 1 and Task 2 to see the difference between terms and stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new function to incorporate the stemming algorithm\n",
    "# Function used from Task 3 of the previous week\n",
    "def parse_doc_stemmed(input_path, stop_words):\n",
    "    my_file = open(input_path, 'r')\n",
    "\n",
    "    stopwords_file = open(stop_words, 'r')\n",
    "    stop_words_list = stopwords_file.readlines()\n",
    "    stopwords_file.close()\n",
    "\n",
    "    stop_words_list = stop_words_list[0].split(',')\n",
    "\n",
    "    word_count = 0\n",
    "    docid = ''\n",
    "    curr_doc = {}\n",
    "\n",
    "    start_end = False\n",
    "    parsed_text = []\n",
    "    \n",
    "\n",
    "    file_ = my_file.readlines()\n",
    "\n",
    "    for line in file_:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith('<text>'):\n",
    "            start_end = True\n",
    "        if line.startswith('<newsitem '):\n",
    "            for part in line.split():\n",
    "                if part.startswith('itemid='):\n",
    "                    docid = part.split('=')[1].split('/')[0]\n",
    "                    docid = docid.replace('\"', '')\n",
    "        elif line.startswith('<p>'):\n",
    "            line = line.replace('<p>', '').replace('</p>', '')\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "            line = line.replace('quot', '')\n",
    "        elif line.startswith('</text>'):\n",
    "            start_end = False\n",
    "        if start_end:\n",
    "            parsed_text.append(line)\n",
    "\n",
    "    split_text = []\n",
    "\n",
    "    for line in parsed_text:\n",
    "        for word in line.split(): \n",
    "            word_count += 1\n",
    "\n",
    "            if word.lower() not in stop_words_list and not word.isdigit():\n",
    "                word = word.lower()\n",
    "                split_text.append(stem(word))\n",
    "\n",
    "    split_text.remove('<text>')\n",
    "    for word in split_text:\n",
    "        if word not in curr_doc:\n",
    "            curr_doc[word] = 1\n",
    "        else:\n",
    "            curr_doc[word] += 1\n",
    "    \n",
    "    my_file.close()\n",
    "    return (word_count, docid, curr_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing outputs between functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Stemming:  {'argentine': 1, 'bonds': 1, 'slightly': 1, 'higher': 1, 'small': 1, 'technical': 2, 'bounce': 2, 'wednesday': 1, 'amid': 1, 'low': 1, 'volume': 1, 'trader': 2, 'large': 1, 'foreign': 1, 'bank': 1, 'slight': 1, 'opening': 1, 'expect': 1, 'prices': 1, 'change': 1, 'much': 1, 'during': 1, 'session': 1, 'marketmoving': 1, 'news': 1, 'expected': 2, 'percent': 1, 'dollardenominated': 1, 'bocon': 1, 'previsional': 1, 'due': 2, 'rose': 2, 'argentinas': 2, 'frb': 1, 'general': 1, 'uncertainty': 1, 'pointing': 1, 'events': 1, 'market': 1, 'waiting': 1, 'including': 1, 'passage': 1, 'governments': 1, 'new': 1, 'economic': 1, 'measures': 1, 'through': 1, 'congress': 1, 'now': 1, 'until': 1, 'early': 1, 'october': 1, 'addition': 1, 'traders': 1, 'awaiting': 1, 'meeting': 1, 'friday': 1, 'between': 1, 'economy': 1, 'minister': 1, 'roque': 1, 'fernandez': 1, 'international': 1, 'monetary': 1, 'fund': 1, 'delegation': 1, 'fiscal': 1, 'deficit': 1, 'axel': 1, 'bugge': 1, 'buenos': 1, 'aires': 1, 'newsroom': 1}\n",
      "Stemming:  {'argentin': 1, 'bond': 1, 'slight': 2, 'higher': 1, 'small': 1, 'technic': 2, 'bounc': 2, 'wednesday': 1, 'amid': 1, 'low': 1, 'volum': 1, 'trader': 3, 'larg': 1, 'foreign': 1, 'bank': 1, 'open': 1, 'expect': 3, 'price': 1, 'chang': 1, 'much': 1, 'dure': 1, 'session': 1, 'marketmov': 1, 'news': 1, 'percent': 1, 'dollardenomin': 1, 'bocon': 1, 'prevision': 1, 'due': 2, 'rose': 2, 'argentina': 2, 'frb': 1, 'general': 1, 'uncertainti': 1, 'point': 1, 'event': 1, 'market': 1, 'wait': 1, 'includ': 1, 'passag': 1, 'govern': 1, 'new': 1, 'econom': 1, 'measur': 1, 'through': 1, 'congress': 1, 'now': 1, 'until': 1, 'earli': 1, 'octob': 1, 'addit': 1, 'await': 1, 'meet': 1, 'friday': 1, 'between': 1, 'economi': 1, 'minist': 1, 'roqu': 1, 'fernandez': 1, 'intern': 1, 'monetari': 1, 'fund': 1, 'deleg': 1, 'fiscal': 1, 'deficit': 1, 'axel': 1, 'bugg': 1, 'bueno': 1, 'air': 1, 'newsroom': 1}\n"
     ]
    }
   ],
   "source": [
    "terms_no_stemming = parse_doc('wk3_workshop_data/6146.xml', 'wk3_workshop_data/common-english-words.txt')[2]\n",
    "\n",
    "parsed_text_task_2 = parse_doc_stemmed('wk3_workshop_data/6146.xml', 'wk3_workshop_data/common-english-words.txt')\n",
    "terms_stemming = parsed_text_task_2[2]\n",
    "\n",
    "print('No Stemming: ', terms_no_stemming)\n",
    "print('Stemming: ', terms_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Defining a Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc_Node:\n",
    "    def __init__(self, data, next = None):\n",
    "        self.data = data\n",
    "        self.next = next\n",
    "\n",
    "class List_Docs:\n",
    "    def __init__(self, hnode = None):\n",
    "        self.head = hnode\n",
    "\n",
    "    def insert(self, nnode):\n",
    "        if not self.head:\n",
    "            self.head = nnode\n",
    "            return\n",
    "        current = self.head\n",
    "        while current.next:\n",
    "            current = current.next\n",
    "        current.next = nnode\n",
    "\n",
    "        return nnode\n",
    "    \n",
    "    def lprint(self):\n",
    "        current = self.head\n",
    "        if not current: \n",
    "            print('List is empty')\n",
    "            return\n",
    "        \n",
    "        while current: \n",
    "            data = current.data\n",
    "            term_count = data[0]\n",
    "            termid = data[1]\n",
    "\n",
    "            print(f'(ID-{termid}: {term_count} terms)')\n",
    "            current = current.next\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Design a main function to read a set of xml files and represent eac file as a node, then create a linked list to link all nodes together. You need to update function `parse_doc()`, such as arguments or return value, and then use `Doc_Node` and `List_Docs` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ID-6146: 137 terms)\n",
      "(ID-741299: 191 terms)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.List_Docs at 0x107608690>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = parse_doc_stemmed('wk3_workshop_data/741299newsML.xml', 'wk3_workshop_data/common-english-words.txt')\n",
    "\n",
    "doc1 = Doc_Node(parsed_text_task_2)\n",
    "doc2 = Doc_Node(new_document)\n",
    "list = List_Docs()\n",
    "\n",
    "list.insert(doc1)\n",
    "list.insert(doc2)\n",
    "\n",
    "list.lprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifn647_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
